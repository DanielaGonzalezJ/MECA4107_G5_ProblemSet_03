my_name
paste(my_name, collapse = " ")
paste("Hello", "world!", sep = " ")
paste(c(1:3),c("X","Y","Z"), sep = "")
paste(LETTERS,1:4 sep = "-")
paste(LETTERS,1:4, sep = "-")
x <- c(44, NA, 5, NA)
x*3
y <- rnorm(1000)
z <- rep(NA, 1000)
my_data <- sample(c(y,z), 100)
is.na(my_data)
my_na <- is.na(my_data)
my_na
my_data == NA
sum(my_na)
my_data
0/0
Inf - inf
Inf - Inf
x
x[1:10]
x[is.na(x)]
y <- x[!is.na(x)]
y
y[y > 0]
x[x > 0]
x[!is.na(x) & x > 0]
x[c(3,5,7)]
x[0]
x[3000]
x[c(-2,-10)]
x[-c(2,10)]
vect <- c(foo = 11, bar = 2, norf = NA)
vect
names(vect)
vect2 <- c(11,2,NA)
names(vect2) <- c("foo", "bar", "norf")
identical(vect, vect2)
vect["bar"]
vect[c("foo", "bar")]
my_vector <- c(1:20)
my_vector <- 1:20
my_vector
dim(my_vector)
length(my_vector)
dim(my_vector) <- c(4,5)
dim(my_vector)
attributes(my_vector)
my_vector
class(my_vector)
my_matrix <- my_vector
?matrix
matrix(data = 1:20, ncol = 5, nrow = 4, dimnames = NULL)
my_matrix2 <- matrix(data = 1:20, ncol = 5, nrow = 4, dimnames = NULL)
identical(my_matrix, my_matrix2)
patientes <- c("Bill", "Gina", "Kelly", "Sean")
patients <- c("Bill", "Gina", "Kelly", "Sean")
cbind(patients, my_matrix)
my_data <- data.frame(patients, my_matrix)
my_data
class(my_data)
cnames <- c("patient", "age", "weight", "bp", "rating", "test")
colnames(cnames, my_data)
colnames(my_data) <- cnames
my_data
q()
install.packages("tidyverse")
q()
install.packages("tidyverse")
q()
install.packages("tidyverse")
install.packages("tidyverse")
install.packages("tidyverse")
q()
library("tidyverse")
library("swirl")
swirl()
q()
library("swirl")
library("tidyverse")
sat <- read.csv("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/2012_SAT_Results.csv")
demog<- read_csv("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/demog.csv")
View(demog)
View(demog)
str(demog)
demog <- demog %>%
select(DBN, Name, schoolyear, frl_percent, total_enrollment, asian_per, black_per, hispanic_per, white_per,starts_with("grade"),selfcontained_num)
str(demog)
demog <- demog %>%
select (-selfcontained_num)
str(demog)
demog <- demog %>% arrange(desc(total_enrollment), desc(white_per))
head(demog)
demog_5 <- demog %>% slice(1:5)
head(demog_5)
demog <- demog %>% filter(grade9 != "NA")
demog <- demog %>% filter(grade9 != "NA", schoolyear == "20112012")
demog_sub <- demog %>% filter(Name %in% c("WILLIAMSBURG PREPARATORY SCHOOL","THE SCHOOL FOR HUMAN RIGHTS","THE HIGH SCHOOL FOR GLOBAL CITIZENSHIP"))
table(demog_sub$Name)
demog <- demog %>% rename(AnoEscolar= schoolyear)
demog <- demog %>% mutate(perc_total= asian_per+black_per+hispanic_per+white_per)
summary(demog$perc_total)
demog_long <- demog %>%
select(DBN, AnoEscolar,Name,asian_per,black_per,hispanic_per,white_per) %>%
pivot_longer(cols = c(asian_per,black_per,hispanic_per,white_per),
names_to = "Race",
values_to = "Perc",
values_drop_na = T)
head(demog_long)
demog_long_sum <- demog_long %>%
group_by(DBN, AnoEscolar,Name) %>%
summarize(perc_total=sum(Perc))
View(demog_long_sum)
sat_demog_dir <- sat %>%
left_join(demog, by = c("DBN"))
str(sat_demog_dir)
View(sat_demog_dir)
plot(sat_demog_dir$SAT.Math.Avg..Score,sat_demog_dir$black_per)
plot(sat_demog_dir$SAT.Math.Avg..Score,sat_demog_dir$white_per)
q()
installed.packages("islr2")
library("islr2")
installed.packages("ISLR2")
installed.packages("ISLR")
?skim()
?skim
q()
installed.packages("stargazaer")
installed.packages("devtools")
install.packages("stargazer")
install.packages("devtools")
q()
require(pacman)
p_load(tidyverse,fixest, stargazer,knitr,kableExtra,jtools,ggstance,broom,broom.mixed,skimr)
install.packages("pacman")
?pacman
library("tidyverse")
load(url("https://github.com/ignaciomsarmiento/datasets/blob/main/bike.RData?raw=true"))
?predict
arg(predict)
library("rio")
library("tidyverse")
install.packages("pacman")
install.packages("rio")
install.packages("pacman")
install.packages("rio")
library("tidyverse")
read_csv("https://raw.githubusercontent.com/#Housekeeping")
nlsy <- read_csv("https://raw.githubusercontent.com")
View(nlsy)
?import
q()
istall.packages("pacman")
install.packages("pacman")
library("rvest")
args(rvest)
args("rvest")
library("rvest")
prueba <- read_html("https://scrapeme.live/shop")
View(prueba)
View(prueba)
library("dyplr")
library("Tidyverse")
library("Tydiverse")
library(tidyverse)
html_products <- document %>% html_elements("li.product")
html_products <- prueba %>% html_elements("li.product")
View(html_products)
View(html_products)
a_element <- html_product %>% html_element("a")
a_element <- html_products %>% html_element("a")
View(a_element)
View(a_element)
product_urls <- html_products %>%
html_element("a") %>%
html_attr("href")
products <- as.tibble(product_urls)
View(products)
View(products)
rm(list = ls())
web <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
htlm <- read_html("web")
web <- "https:\\ignaciomsarmiento.github.io\\GEIH2018_sample\\"
htlm <- read_html("web")
web <- read_html("https:\\ignaciomsarmiento.github.io\\GEIH2018_sample\\")
web <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/")
View(web)
View(web)
links <- htlm %>%
html_nodes("a") %>%
html_attr("href")
links <- web %>%
html_nodes("a") %>%
html_attr("href")
library("tidyverse")
library("rvest")
library("rvest")
web <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/")
links <- web %>%
html_nodes("a") %>%
html_attr("href")
view(links)
print(links)
links <- links %>% filter([6:16])
links <- links %>% filter(6:16)
info_links <- list()
for(link in links){
linked page <- read_html(link)
info_links <- list()
for(link in links){
linked_page <- read_html(link)
}
view(link)
view(links)
install.packages("rio")
rm(list = ls())
p_load(tidyverse, rvest)
library("tidyverse", "rvest")
web <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
class(web)
table_u = list()
table = list()
table_u <- rbind(table_u, table)
for (i in 1:10) {
url <- paste0(web, i, "html")
print(url)
my_web <- read_html(url)
table <- my_web %>%
html_table()
table <- as.data.frame(table)
table_u <- rbind(table_u, table)
}
for (i in 1:10) {url <- paste0(web, i, "html")}
rm(list = ls())
library("tydiverse", "rvest")
library("tidyverse", "rvest")
web <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/"
table_u <- list()
table <- list()
for (i in 1:10) {
url <- paste0(web, i, "html")
print(url)
my_web <- read_html(url)
table <- my_web %>%
html_table()
table <- as.data.frame(table)
table_u <- rbind(table_u, table)
}
view(web)
view(url)
rm(list = ls())
my_url_base <- "https://ignaciomsarmiento.github.io/GEIH2018_sample/page"
my_url_pages <- c(1:10)  # Create a vector of page numbers (integers)
my_url_ext <- ".html"
my_urls <- paste0(my_url_base, my_url_pages, my_url_ext)  # Concatenate all elements
for (i in 1:length(my_urls)) {
url <- my_urls[i]
#browseURL(url)  # Print the full URL
my_html = read_html(url)
class(my_html) ## ver la clase del objeto
#view(my_html)
}
View(my_html)
my_html %>% html_elements("h4")
view(my_html)
rm(list = ls())
web <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/")
##links##
links = list()
links <- web %>%
html_nodes("a") %>%
html_attr("href")
view(links)
view(links)
to_remove <- c("index.html", "https://microdatos.dane.gov.co/index.php/catalog/547/get_microdata", "https://sites.google.com/view/manuelfernandezsierra
", "dictionary.html
", "ddi-documentation-spanish-608.pdf
", "labels.html
")
links_filtrados <- links[!links %in% to_remove]
print(links_filtrados)
links_filtrados <- c("https://sites.google.com/view/manuelfernandezsierra", "dictionary.html", "ddi-documentation-spanish-608.pdf", "labels.html")
links_filtrados1 <- links_filtrados[!links_filtrados %in% to_remove]
print(to_remove)
rm(list = list())
rm(list =ls())
web <- read_html("https://ignaciomsarmiento.github.io/GEIH2018_sample/")
##links##
links = list()
links <- web %>%
html_nodes("a") %>%
html_attr("href")
print(links)
to_remove <- c("index.html", "https://microdatos.dane.gov.co/index.php/catalog/547/get_microdata", "https://sites.google.com/view/manuelfernandezsierra", "dictionary.html", "ddi-documentation-spanish-608.pdf", "labels.html" )
links_1 <- links[!links %in% to_remove]
for(link in links_1){
linked_page <- read_html(link)
info_links <- list()
for(link in links_1){
linked_page <- read_html(link)
}
info_links <- list()
info_links <- list()
info_links = list()
sdf
rm(ls())
rm(list= ls())
q()
sdf
8+3
library("cars")
library("car")
install.packages
install.packages("car")
install.packages("rio")
if(!require(pacman)) install.packages("pacman") ; require(pacman)
p_load(tidyverse,
skimr,
readxl,
sandwich,
lmtest,
boot,
car,
estimatr)
install.packages("rio")
install.packages("caret")
library(stargazer)
q()
install.packages("glmnet")
library(pacman)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, # Manipular dataframes
rio, # Importar datos fácilmente
sf, # Leer/escribir/manipular datos espaciales
tidymodels, # Modelado de datos limpios y ordenados
rattle, # Interfaz gráfica para el modelado de datos
spatialsample) # Muestreo espacial para modelos de aprendizaje automático
library(tidymodels)
install.packages("tidyr")
install.packages("tidyr")
library(pacman)
# Cargar las librerías listadas e instalarlas en caso de ser necesario
p_load(tidyverse, # Manipular dataframes
rio, # Importar datos fácilmente
sf, # Leer/escribir/manipular datos espaciales
tidymodels, # Modelado de datos limpios y ordenados
rattle, # Interfaz gráfica para el modelado de datos
spatialsample) # Muestreo espacial para modelos de aprendizaje automático
db <- import("https://raw.githubusercontent.com/ignaciomsarmiento/datasets/main/house_priceCALI_Clean.rds")
# Todo en minuscula
db <- db %>%
mutate(description = str_to_lower(description))
# Eliminamos tildes
db <- db %>%
mutate(description = iconv(description, from = "UTF-8", to = "ASCII//TRANSLIT"))
# Eliminamos caracteres especiales
db <- db %>%
mutate(description = str_replace_all(description, "[^[:alnum:]]", " "))
# Eliminamos espacios extras
db <- db %>%
mutate(description = str_trim(gsub("\\s+", " ", description)))
# Se crea una nueva columna llamada property_type_2. Si "casa" está en la descripción, se asigna "Casa" a esta columna; de lo contrario, se mantiene el valor original de property_type
db <- db %>%
mutate(property_type_2 = ifelse(grepl("casa", description), "Casa", property_type))
# Se repite el caso anterior pero ahora buscamos apartamento o apto.
db <- db %>%
mutate(property_type_2 = ifelse(grepl("apto|apartamento", description), "Apartamento", property_type_2))%>%
select(-property_type)
db <- db %>%
mutate(n_pisos= str_extract(description, "(\\w+|\\d+) pisos")) %>%
mutate(n_pisos= ifelse(property_type_2=="Casa", n_pisos, NA))
numeros_escritos <- c( "dos", "tres", "cuatro", "cinco", "seis", "siete", "ocho", "nueve", "diez")
numeros_numericos <- as.character(2:10)
db <- db %>%
mutate(n_pisos = str_replace_all(n_pisos, setNames(numeros_numericos,numeros_escritos)))
db <- db %>%
mutate(n_pisos_numerico = as.integer(str_extract(n_pisos, "\\d+")))  %>%
mutate(n_pisos_numerico = if_else(is.na(n_pisos_numerico), 1, n_pisos_numerico)) %>%
mutate(n_pisos_numerico = if_else(n_pisos_numerico>10, 1, n_pisos_numerico)) ### quedarnos casas de hasta 10 pisos
ggplot(db %>% filter(n_pisos_numerico>1), aes(x = factor(n_pisos_numerico))) +
geom_bar() +
labs(title = "", x = "Pisos", y = "Obs")+
theme_minimal()
db <- db %>%
mutate(piso_info= str_extract(description, "(\\w+|\\d+) piso (\\w+|\\d+)"))
numeros_escritos <- c("uno|primero|primer", "dos|segundo|segund", "tres|tercero|tercer", "cuatro|cuarto", "cinco|quinto", "seis|sexto", "siete|septimo", "ocho|octavo", "nueve|noveno", "diez|decimo|dei")
numeros_numericos <- as.character(1:10)
db <- db %>%
mutate(piso_info = str_replace_all(piso_info, setNames(numeros_numericos,numeros_escritos)))
db <- db %>%
mutate(piso_numerico = as.integer(str_extract(piso_info, "\\d+")))
db <- db %>%
mutate(piso_numerico = ifelse(piso_numerico > 20, NA, piso_numerico)) %>%
mutate(piso_numerico = ifelse(property_type_2=="Casa", 1, piso_numerico))
db %>%
filter(property_type_2 == "Apartamento") %>%
count(piso_numerico)
db <- db %>%
mutate(piso_numerico = replace_na(piso_numerico, 1))
ggplot(db %>% filter(piso_numerico>1), aes(x = factor(piso_numerico))) +
geom_bar() +
labs(title = "", x = "Pisos", y = "Obs")+
theme_minimal()
test <- db %>% subset( l4 == 'Lili' )
train <- db %>% subset( l4 != 'Lili' | is.na(l4)==TRUE )
elastic_net_spec <- linear_reg(penalty = tune(), mixture = tune()) %>%
set_engine("glmnet")
grid_values <- grid_regular(penalty(range = c(-2,1)), levels = 50) %>%
expand_grid(mixture = c(0, 0.25,  0.5, 0.75,  1))
# Primera receta
rec_1 <- recipe(price ~ distancia_parque + area_parque + rooms + bathrooms + property_type_2+ piso_numerico+ n_pisos_numerico , data = train) %>%
step_interact(terms = ~ distancia_parque:property_type_2+area_parque:property_type_2) %>% # creamos interacciones con el tipo de propiedad
step_interact(terms = ~ distancia_parque:piso_numerico) %>% # Crea interacción de  la distancia al parque con el piso donde se encuentra el apto.
step_novel(all_nominal_predictors()) %>%   # para las clases no antes vistas en el train.
step_dummy(all_nominal_predictors()) %>%  # crea dummies para las variables categóricas
step_zv(all_predictors()) %>%   #  elimina predictores con varianza cero (constantes)
step_normalize(all_predictors())  # normaliza los predictores.
# Segunda receta
rec_2 <- recipe(price ~  distancia_parque + area_parque + rooms + bathrooms + property_type_2+ piso_numerico+ n_pisos_numerico, data = train) %>%
step_interact(terms = ~ distancia_parque:property_type_2+area_parque:property_type_2) %>%
step_interact(terms = ~ distancia_parque:piso_numerico) %>%
step_interact(terms = ~ distancia_parque:n_pisos_numerico) %>%   # añadimos una interacción de distancia al parque con el número de pisos de la casa.
step_poly(distancia_parque, area_parque, degree = 2) %>%
step_novel(all_nominal_predictors()) %>%
step_dummy(all_nominal_predictors()) %>%
step_zv(all_predictors()) %>%
step_normalize(all_predictors())
workflow_1 <- workflow() %>%
# Agregar la receta de preprocesamiento de datos. En este caso la receta 1
add_recipe(rec_1) %>%
# Agregar la especificación del modelo de regresión Elastic Net
add_model(elastic_net_spec)
## Lo mismo con la receta rec_2
workflow_2 <- workflow() %>%
add_recipe(rec_2) %>%
add_model(elastic_net_spec)
# definimos nuestra variable como sf
train_sf <- st_as_sf(
train,
# "coords" is in x/y order -- so longitude goes first!
coords = c("lon", "lat"),
# Set our coordinate reference system to EPSG:4326,
# the standard WGS84 geodetic coordinate reference system
crs = 4326
)
set.seed(86936)
block_folds <- spatial_block_cv(train_sf, v = 5)
autoplot(block_folds)
p_load("purrr")
walk(block_folds$splits, function(x) print(autoplot(x)))
set.seed(86936)
tune_res1 <- tune_grid(
workflow_1,         # El flujo de trabajo que contiene: receta y especificación del modelo
resamples = block_folds,  # Folds de validación cruzada espacial
grid = grid_values,        # Grilla de valores de penalización
metrics = metric_set(mae)  # metrica
)
collect_metrics(tune_res1)
set.seed(86936)
tune_res2 <- tune_grid(
workflow_2,         # El flujo de trabajo que contiene: receta y especificación del modelo
resamples = block_folds,  # Folds de validación cruzada
grid = grid_values,        # Grilla de valores de penalización
metrics = metric_set(mae)  # metrica
)
collect_metrics(tune_res2)
# Utilizar 'select_best' para seleccionar el mejor valor.
best_tune_res1 <- select_best(tune_res1, metric = "mae")
best_tune_res1
# Utilizar 'select_best' para seleccionar el mejor valor.
best_tune_res2<- select_best(tune_res2, metric = "mae")
best_tune_res2
# Finalizar el flujo de trabajo 'workflow' con el mejor valor de parametros
res1_final <- finalize_workflow(workflow_1, best_tune_res1)
# Ajustar el modelo  utilizando los datos de entrenamiento
res2_final <- finalize_workflow(workflow_2, best_tune_res2)
EN_final1_fit <- fit(res1_final, data = train)
EN_final2_fit <- fit(res2_final, data = train)
augment(EN_final1_fit, new_data = test) %>%
mae(truth = price, estimate = .pred)
augment(EN_final2_fit, new_data = test) %>%
mae(truth = price, estimate = .pred)
rm(list = ls())
if(!require(pacman)) install.packages("pacman") ; require(pacman)
p_load( tidyverse, # tidy-data
glmnet, # To implement regularization algorithms.
caret, # creating predictive models
skimr,
knitr,
kableExtra,
leaflet, # Mapas interactivos
visdat, #Visualizaci´pon de missings
osmdata,
tmaptools,
sf,
stringr,
stargazer
)
#Seleccionamos el directorio y Cargamos las bases de datos
setwd("C://Users//AlfredoRP\OneDrive - INALDE Business School - Universidad de La Sabana//Attachments//Economia//ML//Problem set 3//PS3//MECA4107_G5_ProblemSet_03//Data")
#Seleccionamos el directorio y Cargamos las bases de datos
setwd("C://Users//AlfredoRP//OneDrive - INALDE Business School - Universidad de La Sabana//Attachments//Economia//ML//Problem set 3//PS3//MECA4107_G5_ProblemSet_03//Data")
#Cargamos las bases de Datos
train<-read.csv("trainfiltrado.csv")
test<-read.csv("testfiltrado.csv")
template <- read.csv("submission_template.csv")
install.packages("nnet")
install.packages("nnet")
install.packages("keras")
View(train)
View(train)
